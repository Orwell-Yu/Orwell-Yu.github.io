{"meta":{"title":"mio的博客","subtitle":null,"description":"一个二次元博客？","author":"Orwell-Yu","url":"https://orwell-yu.github.io"},"pages":[{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2021-11-09T14:13:18.000Z","comments":false,"path":"about/index.html","permalink":"https://orwell-yu.github.io/about/index.html","excerpt":"","text":"[さくら荘のhojun] 与&nbsp; Mashiro&nbsp; （ 真（ま）白（しろ） ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2021-11-09T14:13:18.000Z","comments":false,"path":"bangumi/index.html","permalink":"https://orwell-yu.github.io/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2021-11-09T14:13:18.000Z","comments":false,"path":"client/index.html","permalink":"https://orwell-yu.github.io/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2021-11-09T14:13:18.000Z","comments":true,"path":"comment/index.html","permalink":"https://orwell-yu.github.io/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2021-11-09T14:13:18.000Z","comments":true,"path":"rss/index.html","permalink":"https://orwell-yu.github.io/rss/index.html","excerpt":"","text":""},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2021-11-09T14:13:18.000Z","comments":false,"path":"donate/index.html","permalink":"https://orwell-yu.github.io/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2021-11-09T14:13:18.000Z","comments":false,"path":"lab/index.html","permalink":"https://orwell-yu.github.io/lab/index.html","excerpt":"","text":"sakura主题balabala","keywords":"Lab实验室"},{"title":"music","date":"2022-08-18T06:18:28.000Z","updated":"2022-08-18T06:22:21.915Z","comments":false,"path":"music/index.html","permalink":"https://orwell-yu.github.io/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2021-11-09T14:13:18.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"https://orwell-yu.github.io/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2021-11-09T14:13:18.000Z","comments":true,"path":"links/index.html","permalink":"https://orwell-yu.github.io/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2021-11-09T14:13:18.000Z","comments":true,"path":"tags/index.html","permalink":"https://orwell-yu.github.io/tags/index.html","excerpt":"","text":""},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2021-11-09T14:13:18.000Z","comments":false,"path":"video/index.html","permalink":"https://orwell-yu.github.io/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"PyTorch初步：LeNet-5模型搭建","slug":"LeNet-5","date":"2022-08-18T04:29:00.000Z","updated":"2022-08-18T06:47:24.847Z","comments":true,"path":"2022/08/18/LeNet-5/","link":"","permalink":"https://orwell-yu.github.io/2022/08/18/LeNet-5/","excerpt":"","text":"Lab5：深度神经网络训练与加速Zhewen Yu 第一部分 LeNet-5模型1.模型搭建1.1初始化网络根据给出的参考图片，定义网络层。1234567891011def __init__(self): super(MyLeNet_5,self).__init__() self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5,padding=2) self.Sigmoid = nn.Sigmoid() self.pool1 = nn.AvgPool2d(kernel_size=2,stride=2) self.conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5) self.pool2 = nn.AvgPool2d(kernel_size=2,stride=2) self.conv3 = nn.Conv2d(in_channels=16,out_channels=120,kernel_size=5) self.flatten = nn.Flatten() self.fc1 = nn.Linear(120,84) self.output = nn.Linear(84,10) 1.其中，由于输入是32*32，实际图片大小是28*28,定义padding为2。2.这里使用sigmoid函数作为激活函数,nn.Sigmoid() 函数是光滑的，便于求导，同时它能够有效的压缩数据幅度。，sigmoid在该模型中效果好于ReLU作为激活函数。3.在最后一次卷积后，图片成为1*1大小，这里考虑将其展开。4.最后设置两个线性连接层，最后输出10个数字。 1.2前向传播网络12345678910def forward(self, x): x = self.Sigmoid(self.conv1(x)) x = self.pool1(x) x = self.Sigmoid(self.conv2(x)) x = self.pool2(x) x = self.conv3(x) x = self.flatten(x) x = self.fc1(x) x = self.output(x) return x 1.这里选择激活前两次卷积的结果，以获得较好的结果。2.根据图片逐步搭建池化层和连接层。 2.模型训练2.1将数据转化为张量格式1234#Data to Tensordata_transform = transforms.Compose( [transforms.ToTensor()]) 矩阵格式的数据在神经网络中要先张量化。 2.2加载MNIST数据集并设定测试集数据123456#MNIST importtrain_dataset=datasets.MNIST(root='./data', train=True ,transform=data_transform,download=True)train_dataloader=Data.DataLoader(dataset=train_dataset,batch_size=16,shuffle=True)#load testtest_dataset=datasets.MNIST(root='./data', train=False ,transform=data_transform,download=True)test_dataloader=Data.DataLoader(dataset=test_dataset,batch_size=16,shuffle=False) 使用DataLoader载入已有的MNIST数据集，其中，batch_size定为16，设定为乱序，载入到当前目录的data文件夹中。 2.3 设置处理设备为GPU1234#device : GPU or CPUdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#import GPUmodel=MyLeNet_5().to(device) 在没有GPU时，也可以由CPU训练。 2.4 定义损失函数、优化器和学习率123456#Loss Functionloss_fn=nn.CrossEntropyLoss()#optimizeroptimizer=optim.SGD(model.parameters(),lr=1e-3,momentum=0.9)# Learning RateLr_scheduler=optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1) 这里的损失函数使用交叉熵函数，优化器使用随机梯度下降法。查询资料得知，momentum参数设定为0.9时训练结果最好。对于学习率，为防止抖动太大，每间隔10轮变为原来的0.1。 2.5 定义训练函数。1234567891011121314151617181920#traindef train(dataloader,model,loss_fn,optimizer): loss,current,n = 0.0, 0.0 ,0 for batch,(X,y) in enumerate(dataloader): #forward X,y=X.to(device),y.to(device) output = model(X) cur_loss = loss_fn(output,y) _, pred = torch.max(output,axis=1) cur_acc=torch.sum(y==pred)/output.shape[0] #backward optimizer.zero_grad() cur_loss.backward() optimizer.step() #计算准确率 loss += cur_loss.item() #累加本批次损失 current += cur_acc.item() n+=1 print(\"train_loss:\"+str(loss/n)) print(\"train_acc:\"+str(current/n)) 通过交叉熵函数返回的损失值，反向传播增加准确率。 2.6 定义验证函数12345678910111213141516171819#验证def val(dataloader,model,loss_fn,i): model.eval() loss,current,n = 0.0, 0.0 ,0 with torch.no_grad(): for batch,(X,y) in enumerate(dataloader): X,y=X.to(device),y.to(device) output = model(X) cur_loss = loss_fn(output,y) _, pred = torch.max(output,axis=1) cur_acc=torch.sum(y==pred)/output.shape[0] loss += cur_loss.item() #累加本批次损失 current += cur_acc.item() n+=1 writer.add_scalar(\"Loss/train\",cur_loss , i) writer.add_scalar(\"Acc/train\", cur_acc, i) print(\"val_loss:\"+str(loss/n)) print(\"val_acc:\"+str(current/n)) return current/n 基本复制训练部分，值得注意的是验证时模型不参与更新。 2.7设定训练循环12345678910111213141516#start trainepoch =50min_acc= 0for i in range(epoch): print(f'round&#123;i+1&#125;\\n-------------') train(train_dataloader,model,loss_fn,optimizer) a = val(test_dataloader,model,loss_fn,i) #save best model if a &gt;min_acc: folder = 'save_model' if not os.path.exists(folder): os.mkdir('save_model') min_acc=a print('save best model') torch.save(model.state_dict(),'save_model/best_model.pth')print('Finished Training') 设定训练50轮次，保存模型数据到best_model文件夹。 2.8使用TensorBoard实现可视化期间也根据TensorBoard官网教程，在训练程序头尾加上相应代码。12345from torch.utils.tensorboard import SummaryWriterwriter = SummaryWriter()# ...中间部分省略writer.flush()writer.close() 关于TensorBoard结果，在4.4中。 3.模型测试3.1 前期导入基本与train.py一致123456#Data to Tensordata_transform = transforms.Compose( [transforms.ToTensor()])# ...中间部分省略model.load_state_dict(torch.load(\"save_model/best_model.pth\")) 从保存模型参数的best_model文件夹中导入模型。 3.2将张量转化为图片并进行20轮测试123456789101112# tensor to picturesshow = ToPILImage()#vertifyfor i in range(20): X,y = test_dataset[i][0],test_dataset[i][1] show(X).show() X = Variable(torch.unsqueeze(X, dim =0).float(),requires_grad=False).to(device) with torch.no_grad(): pred = model(X) predicted, actual = classes[torch.argmax(pred[0])],classes[y] print(f'predicted:\"&#123;predicted&#125;\",actual:\"&#123;actual&#125;\"') 4.模型结果4.1 GPU占用率截图 4.2 训练结束截图 4.3 训练测试截图 4.4 TansorBoard截图可以看到，占用率较为一般，但是实验结果和准确率极高，为1，损失极低，约为0。、 第二部分 参考文献特别鸣谢 哔哩哔哩大学、知乎大学、YouTube UniversityLe-Net 5模型部分https://www.bilibili.com/video/BV11g411u7eL?spm_id_from=333.337.search-card.all.click通过这个视频理解了前向传播、反向传播https://www.bilibili.com/video/BV1SF411K7fK?spm_id_from=333.337.search-card.all.click&amp;vd_source=b7a86eab6c7d442eb274e6592fa32c1f通过这个视频理解了Le-Net5模型原论文https://www.bilibili.com/video/BV1vU4y1A7QJ?spm_id_from=333.337.search-card.all.click通过这个视频逐步理解和搭建了自己的Le-Net5模型骨架https://zhuanlan.zhihu.com/p/362052826通过这篇文章逐步搭建自己的Le-Net5模型骨架","categories":[{"name":"技术","slug":"技术","permalink":"https://orwell-yu.github.io/categories/技术/"}],"tags":[{"name":"web","slug":"web","permalink":"https://orwell-yu.github.io/tags/web/"},{"name":"AI","slug":"AI","permalink":"https://orwell-yu.github.io/tags/AI/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"https://orwell-yu.github.io/categories/技术/"}]}]}